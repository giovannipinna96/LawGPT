{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# union of train dataset for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH_SUMMARIZATION = \"../data/interim/summarization\"\n",
    "ROOT_DESTINATION_PATH = \"../data/processed/summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COSTITUZIONALE_PATH = os.path.join(ROOT_PATH_SUMMARIZATION, \"costituzionale\")\n",
    "EUR_LEX_SUM_IT_PATH = os.path.join(ROOT_PATH_SUMMARIZATION, \"eur-lex-sum_it\")\n",
    "EUROPARL_PATH = os.path.join(ROOT_PATH_SUMMARIZATION, \"euparl\")\n",
    "ITALCASEHOLD = os.path.join(ROOT_PATH_SUMMARIZATION, \"itacasehold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODINGS = [\"utf-8\", \"utf-8-sig\", \"latin-1\", \"iso-8859-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def read_json_file(file_path, enc=\"utf-8\"):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=enc) as file:\n",
    "            data = json.load(file)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            else:\n",
    "                # If the JSON file contains a dictionary or other format, put it into a list\n",
    "                return [data]\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please provide a valid file path.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON format in the file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# file_path = 'example.json'  # Replace this with your JSON file path\n",
    "# result = read_json_file(file_path)\n",
    "# if result:\n",
    "#     print(\"Contents of JSON file as a list:\")\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join(COSTITUZIONALE_PATH, \"massime_and_pronincie.json\")\n",
    "\n",
    "# ../data/interim/summarization/costituzionale/massime_and_pronincie.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the costituzionale dataset\n",
    "costituzionale_list_dataset = read_json_file(\n",
    "    os.path.join(COSTITUZIONALE_PATH, \"massime_and_pronincie.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the euro-lex-sum_it dataset\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "def read_json_line_by_line(file_path):\n",
    "    data = []\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    # Load each line as a JSON object\n",
    "                    data.append(json.loads(line))\n",
    "                    # print(data)  # Or do something else with the data\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON: {e}. Line skipped.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please provide a valid file path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# for enc in ENCODINGS:\n",
    "eur_lex_sum_it_list_dataset = read_json_line_by_line(\n",
    "    os.path.join(EUR_LEX_SUM_IT_PATH, \"train.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the euparl dataset\n",
    "# euparl_list_dataset = read_json_file(\n",
    "#     os.path.join(EUROPARL_PATH, \"train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read italcasehold dataset\n",
    "italcasehold_list_dataset = read_json_file(\n",
    "    os.path.join(ITALCASEHOLD, \"train_Itacasehold.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(costituzionale_list_dataset))\n",
    "print(type(eur_lex_sum_it_list_dataset))\n",
    "print(type(italcasehold_list_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1028\n",
      "792\n"
     ]
    }
   ],
   "source": [
    "print(len(costituzionale_list_dataset))\n",
    "print(len(eur_lex_sum_it_list_dataset))\n",
    "print(len(italcasehold_list_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(costituzionale_list_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['2021', '2010', '2004', '1975', '1991', '1989', '1987', '1965', '1997', '1967', '1958', '1978', '1981', '2012', '1968', '2009', '1994', '2023', '1996', '1961', '1985', '1962', '1963', '2016', '1972', '1964', '1969', '2022', '1999', '2002', '1956', '2020', '1971', '1995', '1990', '2019', '1976', '1957', '1993', '2007', '1977', '2005', '1979', '2003', '1983', '1966', '1984', '1959', '1960', '2000', '1973', '2006', '2015', '1982', '2011', '2017', '1986', '2018', '2013', '2008', '1970', '1998', '2001', '2014', '1980', '1974', '1992', '1988'])\n"
     ]
    }
   ],
   "source": [
    "print(costituzionale_list_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(costituzionale_list_dataset[0][\"2021\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['testo_pronuncia', 'testo_massima'])\n"
     ]
    }
   ],
   "source": [
    "print(costituzionale_list_dataset[0][\"2021\"][\"1\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "costituzionale_final_list_dataset = []\n",
    "for anno in costituzionale_list_dataset[0]:\n",
    "    for num in costituzionale_list_dataset[0][anno]:\n",
    "        costituzionale_final_list_dataset.append(\n",
    "            costituzionale_list_dataset[0][anno][num]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(costituzionale_final_list_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21790"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(costituzionale_final_list_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(costituzionale_final_list_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['testo_pronuncia', 'testo_massima'])\n"
     ]
    }
   ],
   "source": [
    "print(costituzionale_final_list_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['celex_id', 'reference', 'summary'])\n",
      "dict_keys(['url', 'title', 'doc', 'summary', 'materia'])\n"
     ]
    }
   ],
   "source": [
    "print(eur_lex_sum_it_list_dataset[0].keys())\n",
    "print(italcasehold_list_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = []\n",
    "\n",
    "for eur in eur_lex_sum_it_list_dataset:\n",
    "    merge_data.append({\"reference\": eur[\"reference\"], \"summary\": eur[\"summary\"]})\n",
    "\n",
    "for ita in italcasehold_list_dataset:\n",
    "    merge_data.append({\"reference\": ita[\"doc\"], \"summary\": ita[\"summary\"]})\n",
    "\n",
    "for cost in costituzionale_final_list_dataset:\n",
    "    merge_data.append(\n",
    "        {\"reference\": cost[\"testo_pronuncia\"], \"summary\": cost[\"testo_massima\"]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "23610\n"
     ]
    }
   ],
   "source": [
    "print(type(merge_data[0]))\n",
    "print(len(merge_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['reference', 'summary'])\n"
     ]
    }
   ],
   "source": [
    "print(merge_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder_if_not_exists(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "\n",
    "\n",
    "def save_list_of_dicts_to_json(data_list, destination_folder, file_name):\n",
    "    create_folder_if_not_exists(destination_folder)\n",
    "    destination_path = os.path.join(destination_folder, file_name)\n",
    "    try:\n",
    "        with open(destination_path, \"w\") as file:\n",
    "            json.dump(data_list, file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Data successfully saved to {destination_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while saving data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../data/processed/summarization' already exists.\n",
      "Data successfully saved to ../data/processed/summarization/train_union_summarization.json\n"
     ]
    }
   ],
   "source": [
    "save_list_of_dicts_to_json(\n",
    "    merge_data, ROOT_DESTINATION_PATH, \"train_union_summarization.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_json(os.path.join(ROOT_DESTINATION_PATH, \"train_union_summarization.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23610\n"
     ]
    }
   ],
   "source": [
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2276667/3469970038.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_fixed = ds.applymap(convert_lists_to_strings)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def convert_lists_to_strings(x):\n",
    "    if isinstance(x, list):\n",
    "        return \",\".join(map(str, x))\n",
    "    return x\n",
    "\n",
    "\n",
    "df_fixed = ds.applymap(convert_lists_to_strings)\n",
    "\n",
    "ds_data = Dataset.from_pandas(df_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23610 entries, 0 to 23609\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   reference  23610 non-null  object\n",
      " 1   summary    23395 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 369.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_fixed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A  B  C\n",
      "0  1.0  a  x\n",
      "3  4.0  d  w\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame (replace this with your DataFrame)\n",
    "data = {\"A\": [1, 2, None, 4], \"B\": [\"a\", None, \"c\", \"d\"], \"C\": [\"x\", \"y\", \"z\", \"w\"]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Drop rows with at least one null value\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Display the DataFrame without rows containing null values\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['reference', 'summary'],\n",
       "    num_rows: 23610\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# union dataset for semplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH_SEMPLIFICATION = \"../data/interim/simplification\"\n",
    "ROOT_DESTINATION_PATH_SEMPLIFICATION = \"../data/processed/simplification\"\n",
    "\n",
    "ADMIN_IT = \"admin_it\"\n",
    "ADMIN_IT2 = \"admin_it2\"\n",
    "PACCSS_IT = \"paccss_it\"\n",
    "SIMPITIKI = \"simpitiki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def read_json_file(file_path, enc=\"utf-8\"):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=enc) as file:\n",
    "            data = json.load(file)\n",
    "            if isinstance(data, list):\n",
    "                return data\n",
    "            else:\n",
    "                # If the JSON file contains a dictionary or other format, put it into a list\n",
    "                return [data]\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please provide a valid file path.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Invalid JSON format in the file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_it_dataset_op = read_json_file(\n",
    "    os.path.join(ROOT_PATH_SEMPLIFICATION, ADMIN_IT, \"OP.json\")\n",
    ")\n",
    "admin_it_dataset_rd = read_json_file(\n",
    "    os.path.join(ROOT_PATH_SEMPLIFICATION, ADMIN_IT, \"RD.json\")\n",
    ")\n",
    "admin_it_dataset_rs = read_json_file(\n",
    "    os.path.join(ROOT_PATH_SEMPLIFICATION, ADMIN_IT, \"RS.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(admin_it_dataset_op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_it2_dataset = read_json_file(\n",
    "    os.path.join(ROOT_PATH_SEMPLIFICATION, ADMIN_IT2, \"admin-it-l2.json\")\n",
    ")\n",
    "paccss_it_dataset = read_json_file(\n",
    "    os.path.join(ROOT_PATH_SEMPLIFICATION, PACCSS_IT, \"paccss_it.json\")\n",
    ")\n",
    "simpitiki_dataset = read_json_file(\n",
    "    os.path.join(ROOT_PATH_SEMPLIFICATION, SIMPITIKI, \"simpitiki-v2.json\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['original', 'simplification', 'l2_semp'])\n",
      "dict_keys(['Sentence_1', 'Sentence_2', 'Cosine_Similarity', 'Confidence', 'Readability_1', 'Readability_2', '(Readability_1-Readability_2)'])\n",
      "dict_keys(['type', 'origin', 'before', 'after'])\n"
     ]
    }
   ],
   "source": [
    "print(admin_it2_dataset[0].keys())\n",
    "print(paccss_it_dataset[0].keys())\n",
    "print(simpitiki_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_semplification_dataset = []\n",
    "\n",
    "for a2 in admin_it2_dataset:\n",
    "    final_semplification_dataset.append(\n",
    "        {\"original\": a2[\"original\"], \"simplified\": a2[\"simplification\"]}\n",
    "    )\n",
    "    final_semplification_dataset.append(\n",
    "        {\"original\": a2[\"original\"], \"simplified\": a2[\"l2_semp\"]}\n",
    "    )\n",
    "    final_semplification_dataset.append(\n",
    "        {\"original\": a2[\"simplification\"], \"simplified\": a2[\"l2_semp\"]}\n",
    "    )\n",
    "\n",
    "for pacs in paccss_it_dataset:\n",
    "    final_semplification_dataset.append(\n",
    "        {\"original\": pacs[\"Sentence_1\"], \"simplified\": pacs[\"Sentence_2\"]}\n",
    "    )\n",
    "\n",
    "for swi in simpitiki_dataset:\n",
    "    final_semplification_dataset.append(\n",
    "        {\"original\": swi[\"before\"], \"simplified\": swi[\"after\"]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in admin_it_dataset_op[0]:\n",
    "    final_semplification_dataset.append(\n",
    "        {\n",
    "            \"original\": admin_it_dataset_op[0][op][0],\n",
    "            \"simplified\": admin_it_dataset_op[0][op][1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for rs in admin_it_dataset_rs[0]:\n",
    "        final_semplification_dataset.append(\n",
    "            {\n",
    "                \"original\": admin_it_dataset_rs[0][rs][0],\n",
    "                \"simplified\": admin_it_dataset_rs[0][rs][1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "for rd in admin_it_dataset_rd[0]:\n",
    "    final_semplification_dataset.append(\n",
    "        {\n",
    "            \"original\": admin_it_dataset_rd[0][rd][0],\n",
    "            \"simplified\": admin_it_dataset_rd[0][rd][1],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../data/processed/simplification' created.\n",
      "Data successfully saved to ../data/processed/simplification/train_union_simplification.json\n"
     ]
    }
   ],
   "source": [
    "save_list_of_dicts_to_json(\n",
    "    merge_data, ROOT_DESTINATION_PATH_SEMPLIFICATION, \"train_union_simplification.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
